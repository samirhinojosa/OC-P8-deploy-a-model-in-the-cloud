ğŸ‘¥ Note dâ€™accompagnement

Lâ€™intention de ce projet est de permettre Ã  lâ€™Ã©tudiant de mettre en Å“uvre des compÃ©tences de Data Architect, au travers de la mise en place dâ€™une architecture Big Data.

En effet, si votre Ã©tudiant vous questionne sur la pertinence dâ€™acquÃ©rir les compÃ©tences du projet, vous pouvez lui expliquer quâ€™en tant que Data Scientist, il peut Ãªtre amenÃ© Ã  coordonner les actions de diffÃ©rents acteurs, en particulier des Data Architect. La connaissance d'un Cloud de type AWS ou Azure est dâ€™ailleurs souvent exigÃ© par les entreprises.

Attention les ressources gratuites AWS sont limitÃ©es en puissance et capacitÃ© (RAM, CPU, nombre fichiers S3), ce qui oblige Ã  mettre en Å“uvre des configurations et un jeu de dâ€™images adaptÃ©s Ã  ces capacitÃ©s, sans remettre en cause lâ€™objectif du projet :

Envisager un serveur EC2 plus puissant Â« t2.medium Â», 4 GO RAM, 30 Go disque dur (coÃ»t estimÃ© < 10 Euros)
SÃ©lectionner quelques dizaines dâ€™images, parmi au moins 2 familles de fruits, câ€™est suffisant pour tester lâ€™architecture
Exemple dâ€™architecture technique :

Serveur AWS EC2 = exÃ©cution des scripts pyspark dans Jupyter Notebook
Serveur fichier AWS S3 = stockage images initiales et fichiers crÃ©Ã©s par scripts dans S3
AccÃ¨s sÃ©curisÃ© SSH Ã  la console du serveur EC2, via PuTTY (Windows) = accÃ¨s Ã  ligne de commande pour les installations
AccÃ¨s au serveur EC2 via Â« connexion bureau Ã  distance Â» pour travailler sur le Notebook Jupyter
Exemple de librairies Python :

pyspark, version >= 2.3, idÃ©alement 2.4 ou 3.0
accÃ¨s Ã  S3  : par le SDK (boto3) ou lâ€™API (S3a) pour upload et download de fichiers 
Ã‰ventuellement utilisation de fichiers de type Â« parquet Â» pour les sauvegardes dans S3
Le cours sur AWS dÃ©crit la crÃ©ation dâ€™un serveur EC2 et dâ€™un serveur de fichiers S3.

Le choix dâ€™une image disque du systÃ¨me Â« AMI Â» gratuite est assez limitÃ©, prendre par exemple un Â« Ubuntu bionicâ€ version  18.04 ou ultÃ©rieure  et installer Ã  la main tous les composants nÃ©cessaires afin dâ€™avoir les derniÃ¨res versions et la configuration adaptÃ©e (Anaconda, spark, boto3, â€¦).

Il est conseillÃ© Ã  lâ€™Ã©tudiant de tester son notebook PySpark dans un premier temps sur son PC en local (Windows ou Linux) avant de le transfÃ©rer sur lâ€™instance EC2 AWS (Linux).

Si lâ€™Ã©tudiant constate que lâ€™utilisation du Notebook Jupyter Ã  distance est lente (mÃªme avec un t2.medium, 4 Go RAM), ou quâ€™il rencontre des problÃ¨mes de paramÃ©trage, il ou elle peut installer un serveur local (sur PC perso) identique, dans une VM via VirtualBox (installation dâ€™une VM Ubuntu bionic version 18.04 ou ultÃ©rieure), pour rÃ©aliser les tests dâ€™installation, de paramÃ©trage de lâ€™environnement et dâ€™exÃ©cution, avant de le transfert sur le serveur EC2 AWS pour dÃ©monstration. Cette approche, plus coÃ»teuse en temps dâ€™installation, rÃ©duira le coÃ»t financier dâ€™un serveur EC2 t2.medium.

Assurez-vous que lâ€™Ã©tudiant dÃ©veloppe les traitements de pre-processing et de rÃ©duction de dimension en pyspark, dans le cadre dâ€™une Â« sparkSession Â».

Le pre-processing consiste Ã  accÃ©der aux images sur S3, les transformer, et prÃ©parer les matrices des images et labels (type de fruit = nom du dossier associÃ© Ã  lâ€™image).

En cas de dÃ©passement mÃ©moire lors de la rÃ©duction de dimension (PCA), lâ€™Ã©tudiant pourra diminuer la taille et le nombre dâ€™images Ã  traiter (pre-processing), afin dâ€™Ã©viter de passer du temps sur le tuning mÃ©moire des sessions spark (hors scope).

Concernant le dÃ©veloppement du notebook en PySpark, lâ€™Ã©tudiant pourra par exemple utiliser un modÃ¨le CNN prÃ©-entraÃ®nÃ© simple (CNN Transfer Learning), en enlevant la derniÃ¨re couche, afin de crÃ©er des features et rÃ©aliser ainsi naturellement une rÃ©duction de dimension. Il pourra encapsuler cette fonction dans un â€œpandas_udf â€œ afin de maintenir le traitement en mode distribuÃ©.

ğŸ•µï¸â€ Ã‰valuation des compÃ©tences

ğŸ¯Identifier les outils du cloud permettant de mettre en place un environnement Big Data

â’ L'apprenant a identifiÃ© les diffÃ©rentes briques d'architecture nÃ©cessaire pour la mise en place d'un environnement Big Data

â’ L'apprenant a identifiÃ© les outils du cloud permettant de mettre en place l'environnement Big Data

ğŸ¯Utiliser les outils du cloud pour manipuler de la donnÃ©e dans un environnement Big Data

â’ Les fichiers (de dÃ©part et ceux aprÃ¨s transformation) sont chargÃ©s dans un espace de stockage cloud

â’ Les scripts ont Ã©tÃ© exÃ©cutÃ©s en utilisant des machines dans le cloud

â’ Un des scripts permet d'Ã©crire les sorties du programme directement dans l'espace de stockage cloud (s'assurer que toute la chaÃ®ne de traitement peut-Ãªtre exÃ©cutÃ©e dans le cloud et qu'il n'y a pas tÃ©lÃ©chargement des fichiers en local puis chargement Ã  la main des fichiers dans le cloud) 

ğŸ¯ParallÃ©liser des opÃ©rations de calcul avec Pyspark

â’ Les traitements critiques lors d'un passage Ã  l'Ã©chelle en terme de volume de donnÃ©es sont identifiÃ©s

â’ Les scripts sont dÃ©veloppÃ©s en Pyspark 

Respect des consignes

â’ Les livrables sont complets

â’ Les livrables ont Ã©tÃ© dÃ©posÃ©s 48h Ã  l'avance

â’ Le temps de prÃ©sentation est bien gÃ©rÃ© par l'Ã©tudiant